{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c18cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "class StructformerConfig(PretrainedConfig):\n",
    "    model_type = \"structformer\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        num_layers: int = 4,\n",
    "        max_length: int = 128,\n",
    "        vocab_size: int = 50257,\n",
    "        c: float = 1.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.c = c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b181224",
   "metadata": {},
   "outputs": [],
   "source": [
    "spdefault_config = StructformerConfig()\n",
    "spdefault_config.save_pretrained(\"custom_structformer_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc0d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from flax import linen as nn \n",
    "# import jax.numpy as jnp\n",
    "from transformers import PreTrainedModel\n",
    "from typing import Any, Optional\n",
    "from models.hyperbolic_layers import mobius_add\n",
    "\n",
    "class PoincareHierarchicalBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, c=1.0):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(hidden_size, num_heads, batch_first=True)\n",
    "        self.c = c\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Convert mask for key_padding_mask in PyTorch (True = ignore)\n",
    "        attn_mask = ~mask.bool()\n",
    "        attn_output, _ = self.self_attn(x, x, x, key_padding_mask=attn_mask)\n",
    "        hyp_output = mobius_add(x, attn_output, c=self.c)\n",
    "        return hyp_output\n",
    "\n",
    "class StructformerPoincare(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        hidden_dim=512,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        max_length=128,\n",
    "        c=1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_length, hidden_dim))\n",
    "        self.layers = nn.ModuleList(\n",
    "            [PoincareHierarchicalBlock(hidden_dim, num_heads, c) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(hidden_dim)\n",
    "        self.head = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.token_embed(input_ids) + self.pos_embed[:, :input_ids.size(1), :]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask)\n",
    "        x = self.ln(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "class StructformerModel(PreTrainedModel):\n",
    "    config_class = StructformerConfig # your custom config or standard HF config\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = StructformerPoincare(\n",
    "            vocab_size=config.vocab_size,\n",
    "            hidden_dim=getattr(config, \"hidden_dim\", 512),\n",
    "            num_heads=getattr(config, \"num_heads\", 8),\n",
    "            num_layers=getattr(config, \"num_layers\", 6),\n",
    "            max_length=getattr(config, \"max_length\", 128),\n",
    "            c=getattr(config, \"c\", 1.0),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a3d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoModelForMaskedLM\n",
    "\n",
    "AutoConfig.register(\"structformer\", StructformerConfig)\n",
    "AutoModel.register(StructformerConfig, StructformerModel)\n",
    "AutoModelForMaskedLM.register(StructformerConfig, StructformerModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339dc168",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca184e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89decf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "StructformerConfig.register_for_auto_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5421108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "StructformerModel.register_for_auto_class(\"AutoModelForMaskedLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c0e9d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228f7cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_config = StructformerConfig()\n",
    "sp_config.save_pretrained(\"default_structformer_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a8e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bendemonium/babylm-poincare-structformer\",\n",
    "                                             trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import torch\n",
    "\n",
    "# with open(\"data/tokens/test_tokenized.pkl\", \"rb\") as f:\n",
    "#     tokenized_data = pickle.load(f)\n",
    "\n",
    "# input_ids = torch.tensor(tokenized_data[\"input_ids\"])\n",
    "# attention_mask = torch.tensor(tokenized_data[\"attention_mask\"])\n",
    "\n",
    "# model.eval()  # set to evaluation mode\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f23b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1ae52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.num_parameters(only_trainable=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babyLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "-1.-1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
