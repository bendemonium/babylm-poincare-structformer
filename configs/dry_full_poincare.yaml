seed: 42

# ---- Model ----
vocab_name: gpt2
max_length: 64
hidden_dim: 64
num_layers: 2
num_heads: 4
dropout_rate: 0.1
c: 1.0
pad_id: null
attention_input: tangent

# ---- Training ----
training:
  num_epochs: 1
  max_words: 2000000              # short run for dry run
  batch_size: 64
  eval_batch_size: 64
  eval_interval_words: 25000     # tighter eval for fast feedback
  lr_ce: 2.0e-4
  lr_riem: 1.0e-4
  lambda_h: 0.05                  # enable hyperbolic loss
  lambda_tree: 0.05               # (if tree_data provided; else contributes 0)
  tau: 1.0
  mode: structformer_poincare     # <â€” full pipeline

# ---- Data ----
data:
  type: hf
  hf_repo_id: bendemonium/babylm25_bpe_tokens
  repo_type: dataset
  train_split: train[:1%]
  val_split: dev[:1%]

# ---- Checkpointing ----
checkpointing:
  output_repo_id: bendemonium/babylm-poincare-structformer
  branch_prefix: checkpoint_full_dry
  include_modeling_files:
    - models/structformer_poincare.py
    - models/hyperbolic_geometry.py
    - utils/train_utils.py
    - utils/logging_utils.py
    - utils/save_utils.py
  model_file: models/structformer_poincare.py
  use_word_milestones: true
  max_words: 2000000
# ---- Logging ----
logging:
  log_dir: logs/dry_full_poincare
  use_wandb: true
  wandb_project: structformer-flax
  wandb_run_name: dry-full-poincare
  console_log_level: INFO
  file_log_level: DEBUG
  log_interval_words: 10_000       # dense training logs
  eval_log_interval: 1_000
  progress_bar: true
  words_per_second_window: 1000
  loss_spike_threshold: 2.0
  nan_alert: true
  memory_alert_threshold_gb: 8.0