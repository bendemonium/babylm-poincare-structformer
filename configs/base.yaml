# Training settings
num_epochs: 5
batch_size: 128
learning_rate: 1e-4
seq_length: 128
hidden_dim: 256
num_heads: 6
num_layers: 4
seed: 42

# Tokenizer and vocabulary
vocab_name: gpt2

# Hugging Face Datasets (for tokenized input data)
train_tokenized_repo: bendemonium/babylm25_bpe_tokens
train_tokenized_file: train_tokenized.pkl

val_tokenized_repo: bendemonium/babylm25_bpe_tokens
val_tokenized_file: dev_tokenized.pkl