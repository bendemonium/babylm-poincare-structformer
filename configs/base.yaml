num_epochs: 10
batch_size: 64
learning_rate: 1e-4
seq_length: 128
hidden_dim: 256
num_heads: 8
num_layers: 4
vocab_name: gpt2
train_tokenized_file: data/tokens/train_tokenized.pkl
val_tokenized_file: data/tokens/dev_tokenized.pkl
seed: 42