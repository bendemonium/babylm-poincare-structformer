seed: 42

# ---- Model ----
vocab_name: gpt2
max_length: 128
hidden_dim: 256
num_layers: 8
num_heads: 8
dropout_rate: 0.1
c: 1.0
pad_id: null
attention_input: tangent

# ---- Training ----
training:
  num_epochs: 4
  max_words: 100000000            # metadata + for logging
  batch_size: 128
  eval_batch_size: 128
  eval_interval_words: 1000000    # run eval ~every 1M words
  lr_ce: 2.0e-4
  lr_riem: 1.0e-4
  lambda_h: 0.05
  lambda_tree: 0.05
  tau: 1.0
  mode: structformer_poincare     # options: structformer_only / structformer_poincare

# ---- Data ----
data:
  type: pickle                    # "hf" or "pickle"
  hf_repo_id: bendemonium/babylm25_bpe_tokens
  repo_type: dataset
  train_split: train
  val_split: dev
  train_tokenized_repo: bendemonium/babylm25_bpe_tokens
  train_tokenized_file: train_tokenized.pkl
  val_tokenized_repo: bendemonium/babylm25_bpe_tokens
  val_tokenized_file: dev_tokenized.pkl

# ---- Checkpointing ----
checkpointing:
  output_repo_id: bendemonium/babylm-poincare-structformer
  branch_prefix: checkpoint
  include_modeling_files:
    - models/structformer_poincare.py
    - models/hyperbolic_geometry.py
    - utils/train_utils.py
    - utils/logging_utils.py
    - utils/save_utils.py
  model_file: models/structformer_poincare.py
  use_word_milestones: true
  max_words: 100000000

# ---- Logging ----
logging:
  log_dir: logs/structformer_run
  use_wandb: true
  wandb_project: structformer-flax
  wandb_run_name: run-babylm-poincare
  console_log_level: INFO
  file_log_level: DEBUG
  log_interval_steps: 900
  log_interval_words: 500000
  eval_log_interval: 1000
  progress_bar: true
  words_per_second_window: 1000
  loss_spike_threshold: 2.0
  nan_alert: true
  memory_alert_threshold_gb: 8.0