seed: 42

# Dataset and tokenizer settings
data:
  dataset_type: "pickle"  # Using pickle files from HF repos
  train_tokenized_repo: bendemonium/babylm25_bpe_tokens
  train_tokenized_file: train_tokenized.pkl
  val_tokenized_repo: bendemonium/babylm25_bpe_tokens
  val_tokenized_file: dev_tokenized.pkl
  vocab_name: gpt2  # Tokenizer for vocab_size extraction

# Model architecture hyperparameters
model:
  vocab_size: 50257  # GPT-2 tokenizer vocab size
  hidden_dim: 256
  num_heads: 8
  num_layers: 4
  max_length: 128  # renamed from seq_length for consistency
  c: 1.0  # Hyperbolic geometry curvature parameter

# Training hyperparameters - WORD-BASED (not epoch-based)
training:
  max_words: 100_000_000    # stop after this many "words" (words = 0.75 * non-pad tokens)
  batch_size: 128
  eval_batch_size: 64  # Separate eval batch size
  
  # Word-based training limits (BabyLM style) - FIXED
  max_words: 100000000  # 100M words total - THIS WAS MISSING
  eval_interval_words: 1000000  # Run validation every 1M words
  
  # Remove epoch-based settings since we're doing word-based training
  # num_epochs: 5  # REMOVED
  # save_interval: 1  # REMOVED  
  # eval_interval: 1  # REMOVED

# ---- Data ----
data:
  type: pickle                # hf or pickle
  hf_repo_id: bendemonium/babylm25-bpe-tokens
  train_split: train
  val_split: dev 
  # pickle
  train_tokenized_repo: bendemonium/babylm25-bpe-tokens
  train_tokenized_file: train_tokeinzed.pkl
  val_tokenized_repo: bendemonium/babylm25-bpe-tokens
  val_tokenized_file: dev_tokenized.pkl

# ---- Checkpointing ----
checkpointing:
  output_repo_id: bendemonium/babylm-structformer-poincare # HF Hub model repo to push to
  branch_prefix: checkpoint
  include_modeling_files:
    - models/structformer_poincare.py
    - models/hyperbolic_geometry.py
    - utils/train_utils.py
    - utils/logging_utils.py
    - utils/save_utils.py
  model_file: models/structformer_poincare.py
  use_word_milestones: true   # for metadata purposes
  max_words: 100_000_000      # mirrored for save metadata

# ---- Logging ----
logging:
  log_dir: logs/structformer_run
  use_wandb: true
  wandb_project: structformer-flax
  wandb_run_name: run-001
  log_interval_words: 100000  # Log metrics every 100K words

# Checkpoint & Export Configuration - Word milestone based
checkpointing:
  use_word_milestones: true  # Enable word-based checkpointing
  max_words: 100000000  # 100M words
  output_repo_id: "bendemonium/babylm-poincare-structformer" 
  branch_prefix: "checkpoint"  # Will become "checkpoint_1M_words", etc.
  include_modeling_files:
    - "models/structformer_poincare.py"  # FIXED: was missing .py
    - "models/hyperbolic_layers.py"
  model_file: "models/structformer_poincare"

# System Configuration
system:
  seed: 42
  jax_device: "gpu"
  precision: "float32"

# Testing Configuration (for test_utils.py)
testing:
  test_batch_size: 2
  test_seq_len: 8
  test_vocab_size: 100
  test_iterations: 50
