# base.yaml - Configuration for StructformerPoincare Training

# Dataset and tokenizer settings

dataset_type: "pickle"  # Using pickle files from HF repos
train_tokenized_repo: bendemonium/babylm25_bpe_tokens
train_tokenized_file: train_tokenized.pkl
val_tokenized_repo: bendemonium/babylm25_bpe_tokens
val_tokenized_file: dev_tokenized.pkl
vocab_name: gpt2  # Tokenizer for vocab_size extraction

# Model architecture hyperparameters
model:
  vocab_size: 50257  # GPT-2 tokenizer vocab size
  hidden_dim: 256
  num_heads: 8
  num_layers: 4
  max_length: 128  # renamed from seq_length for consistency
  c: 1.0  # Hyperbolic geometry curvature parameter

# Training hyperparameters
training:
  # Dual learning rates for dual-optimizer setup
  lr_ce: 0.0001      # Learning rate for cross-entropy loss (non-embedding params)
  lr_riem: 0.0001    # Learning rate for Riemannian/Poincaré loss (embedding params)
  lambda_poincare: 0.1  # Weight for combining Poincaré distance loss
  
  batch_size: 128
  eval_batch_size: 64  # Separate eval batch size
  num_epochs: 5
  
  # Batch limits per epoch (useful for development/testing)
  train_max_batches: null  # null = use full dataset
  eval_max_batches: 100    # limit eval batches for faster validation
  
  save_interval: 1  # Save checkpoint every N epochs
  eval_interval: 1  # Run validation every N epochs

# Logging and experiment tracking
logging:
  log_dir: "logs/structformer_run"
  use_wandb: true
  wandb_project: structformer-flax
  wandb_run_name: run-001

# Checkpoint & Export Configuration
checkpointing:
  output_repo_id: "bendemonium/babylm-poincare-structformer" 
  branch_prefix: "checkpoint"
  include_modeling_files:
    - "models/hyperbolic_layers.py"
  model_file: "models/structformer_poincare.py"

# System Configuration
system:
  seed: 42
  jax_device: "gpu"
  precision: "float32"

# Testing Configuration (for test_utils.py)
testing:
  test_batch_size: 2
  test_seq_len: 8
  test_vocab_size: 100
  test_iterations: 50
